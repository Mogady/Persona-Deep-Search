# Deep Research AI Agent

> An autonomous multi-agent research system for conducting comprehensive due diligence investigations on individuals and entities.

## ğŸ¯ Project Overview

This AI-powered research agent autonomously investigates targets by intelligently searching the web, extracting facts, validating information across sources, identifying risks, mapping connections, and generating comprehensive reports. Built with LangGraph orchestration and a multi-model AI strategy for optimal performance.

### Key Capabilities

- **Progressive Search Strategy**: Iteratively deepens investigation across 7 iterations (broad â†’ targeted â†’ connections â†’ validation)
- **Multi-Source Validation**: Cross-references facts across multiple sources with confidence scoring
- **Risk Detection**: Automatically identifies legal, financial, reputational, compliance, and behavioral red flags
- **Network Analysis**: Maps relationships between entities, organizations, and events
- **Comprehensive Reporting**: Generates structured markdown reports with evidence citations

### Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Orchestration** | LangGraph | Multi-agent workflow coordination |
| **AI Models** | Claude Sonnet 4.5 | Complex reasoning, risk analysis, reporting |
| | Gemini Pro 2.5 | Fact extraction, entity recognition |
| | Gemini Flash 2.5 | Fast query generation, filtering |
| **Search** | SerpApi | Google Search API integration |
| **Database** | PostgreSQL | State persistence, audit trail |
| **UI** | Chainlit | Interactive web interface |
| **Deployment** | Docker + GCP | Containerized cloud deployment |

---

## ğŸ”„ System Architecture & Flow

### High-Level Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INPUT (Chainlit UI)                          â”‚
â”‚                   "Research: [Target Name]" + Depth (1-7)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LANGGRAPH ORCHESTRATOR                             â”‚
â”‚                     (ResearchWorkflow State Machine)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    CREATE SESSION IN DB       â”‚
                    â”‚  session_id, target, status   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘              ITERATION LOOP (1-7 times)                â•‘
        â•‘                                                        â•‘
        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
        â•‘  â”‚  NODE 1: QUERY PLANNER (Gemini Flash)          â”‚  â•‘
        â•‘  â”‚  â€¢ Iteration 1: Broad discovery queries        â”‚  â•‘
        â•‘  â”‚  â€¢ Iteration 2-3: Targeted investigation       â”‚  â•‘
        â•‘  â”‚  â€¢ Iteration 4-5: Deep connection mining       â”‚  â•‘
        â•‘  â”‚  â€¢ Iteration 6-7: Validation queries           â”‚  â•‘
        â•‘  â”‚  OUTPUT: 3-5 unique search queries             â”‚  â•‘
        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
        â•‘                         â”‚                              â•‘
        â•‘                         â–¼                              â•‘
        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
        â•‘  â”‚  NODE 2: SEARCH EXECUTOR (SerpApi)             â”‚  â•‘
        â•‘  â”‚  â€¢ Parallel query execution (5 workers)        â”‚  â•‘
        â•‘  â”‚  â€¢ Result deduplication (URL + content)        â”‚  â•‘
        â•‘  â”‚  â€¢ Source diversity tracking                   â”‚  â•‘
        â•‘  â”‚  OUTPUT: 10-50 search results                  â”‚  â•‘
        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
        â•‘                         â”‚                              â•‘
        â•‘                         â–¼                              â•‘
        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
        â•‘  â”‚  NODE 3: CONTENT EXTRACTOR (Gemini Pro)        â”‚  â•‘
        â•‘  â”‚  â€¢ Batch extraction (5 results/call)           â”‚  â•‘
        â•‘  â”‚  â€¢ Atomic fact generation                      â”‚  â•‘
        â•‘  â”‚  â€¢ Category classification (4 types)           â”‚  â•‘
        â•‘  â”‚  â€¢ Entity extraction (NER)                     â”‚  â•‘
        â•‘  â”‚  OUTPUT: 5-20 structured facts                 â”‚  â•‘
        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
        â•‘                         â”‚                              â•‘
        â•‘                         â–¼                              â•‘
        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
        â•‘  â”‚  NODE 4: VALIDATOR (Claude Sonnet)             â”‚  â•‘
        â•‘  â”‚  â€¢ Semantic similarity grouping (embeddings)   â”‚  â•‘
        â•‘  â”‚  â€¢ Cross-reference facts across sources        â”‚  â•‘
        â•‘  â”‚  â€¢ Contradiction detection                     â”‚  â•‘
        â•‘  â”‚  â€¢ Confidence adjustment (+/- scoring)         â”‚  â•‘
        â•‘  â”‚  OUTPUT: Validated facts with confidence       â”‚  â•‘
        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
        â•‘                         â”‚                              â•‘
        â•‘                         â–¼                              â•‘
        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
        â•‘  â”‚  NODE 5: RISK ANALYZER (Claude Sonnet)         â”‚  â•‘
        â•‘  â”‚  â€¢ Pattern detection across facts              â”‚  â•‘
        â•‘  â”‚  â€¢ 5 categories: Legal, Financial, etc.        â”‚  â•‘
        â•‘  â”‚  â€¢ Severity classification (Low â†’ Critical)    â”‚  â•‘
        â•‘  â”‚  OUTPUT: Risk flags with evidence              â”‚  â•‘
        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
        â•‘                         â”‚                              â•‘
        â•‘                         â–¼                              â•‘
        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
        â•‘  â”‚  NODE 6: CONNECTION MAPPER (Gemini Pro)        â”‚  â•‘
        â•‘  â”‚  â€¢ Entity relationship extraction              â”‚  â•‘
        â•‘  â”‚  â€¢ 6 types: Employment, Investment, etc.       â”‚  â•‘
        â•‘  â”‚  â€¢ Graph construction (nodes + edges)          â”‚  â•‘
        â•‘  â”‚  OUTPUT: Connection graph                      â”‚  â•‘
        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
        â•‘                         â”‚                              â•‘
        â•‘                         â–¼                              â•‘
        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
        â•‘  â”‚         CHECKPOINT: Save to Database            â”‚  â•‘
        â•‘  â”‚  â€¢ Facts, Risks, Connections, Search History   â”‚  â•‘
        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
        â•‘                         â”‚                              â•‘
        â•‘                         â–¼                              â•‘
        â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
        â•‘  â”‚          CONDITIONAL LOGIC                      â”‚  â•‘
        â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â•‘
        â•‘  â”‚  â”‚ Continue if:                            â”‚   â”‚  â•‘
        â•‘  â”‚  â”‚  â€¢ current_iteration < research_depth   â”‚   â”‚  â•‘
        â•‘  â”‚  â”‚  â€¢ new facts discovered                 â”‚   â”‚  â•‘
        â•‘  â”‚  â”‚  â€¢ not manually stopped                 â”‚   â”‚  â•‘
        â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â•‘
        â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
        â•‘            YES â—„â”€â”€â”€â”€â”€â”€â”€â”˜                              â•‘
        â•‘             â”‚                                         â•‘
        â•‘             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LOOP BACK TO NODE 1        â•‘
        â•‘                                                       â•‘
        â•‘            NO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                        â”‚
                                        â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  NODE 7: REPORT GENERATOR         â”‚
                    â”‚         (Claude Sonnet)            â”‚
                    â”‚  â€¢ Executive summary               â”‚
                    â”‚  â€¢ Subject overview                â”‚
                    â”‚  â€¢ Facts by category               â”‚
                    â”‚  â€¢ Risk assessment                 â”‚
                    â”‚  â€¢ Network analysis                â”‚
                    â”‚  â€¢ Timeline                        â”‚
                    â”‚  â€¢ Recommendations                 â”‚
                    â”‚  OUTPUT: Markdown report           â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    SAVE FINAL STATE TO DB         â”‚
                    â”‚  â€¢ Complete session                â”‚
                    â”‚  â€¢ Final report text               â”‚
                    â”‚  â€¢ Connection graph JSON           â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     RETURN TO USER (Chainlit)     â”‚
                    â”‚  â€¢ Display formatted report       â”‚
                    â”‚  â€¢ Show facts table               â”‚
                    â”‚  â€¢ Visualize connections          â”‚
                    â”‚  â€¢ Download options (MD/JSON)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### State Flow Example

**Input**: Research "Elon Musk" with depth=7

```
Iteration 1: Broad Discovery
  â”œâ”€ Queries: ["Elon Musk biography", "Elon Musk companies", "Elon Musk news"]
  â”œâ”€ Search: 30 results from diverse sources
  â”œâ”€ Extract: 15 facts (Tesla CEO, SpaceX founder, etc.)
  â””â”€ State: 15 facts collected â†’ CONTINUE

Iteration 2: Targeted Investigation
  â”œâ”€ Queries: ["Elon Musk Tesla compensation", "SpaceX Starship development"]
  â”œâ”€ Extract: 12 new facts (compensation details, SpaceX milestones)
  â””â”€ State: 27 facts collected â†’ CONTINUE

Iteration 3: Targeted Investigation
  â”œâ”€ Queries: ["Elon Musk Twitter acquisition", "Neuralink FDA approval"]
  â”œâ”€ Extract: 10 new facts
  â””â”€ State: 37 facts collected â†’ CONTINUE

Iteration 4: Deep Connection Mining
  â”œâ”€ Queries: ["Elon Musk board positions", "Musk business partnerships"]
  â”œâ”€ Connections: Tesla-SolarCity, PayPal cofounders, etc.
  â””â”€ State: 45 facts, 8 connections â†’ CONTINUE

Iteration 5: Deep Connection Mining
  â”œâ”€ Queries: ["Elon Musk investors", "Musk family background"]
  â”œâ”€ Connections: Peter Thiel, venture capital relationships
  â””â”€ State: 52 facts, 15 connections â†’ CONTINUE

Iteration 6: Validation & Gap Filling
  â”œâ”€ Queries: ["Elon Musk SEC lawsuit details", verify low-confidence facts]
  â”œâ”€ Risks: SEC violations (Medium severity)
  â””â”€ State: 58 facts, 15 connections, 3 risks â†’ CONTINUE

Iteration 7: Final Validation
  â”œâ”€ Queries: Verify remaining low-confidence facts
  â”œâ”€ Final validation and cross-referencing
  â””â”€ State: 61 facts, 18 connections, 4 risks â†’ REPORT

Final Report: Comprehensive markdown with all findings
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Python 3.11+**
- **PostgreSQL 14+** (or Docker)
- **API Keys**:
  - Anthropic API key (Claude Sonnet)
  - Google AI API key (Gemini Pro & Flash)
  - SerpApi key (Google Search)

### 1. Clone Repository

```bash
git clone <repository-url>
cd Elile-Assessment
```

### 2. Set Up Virtual Environment

```bash
# Create virtual environment
python -m venv .venv

# Activate virtual environment
# On Linux/Mac:
source .venv/bin/activate
# On Windows:
.venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Configure Environment Variables

```bash
# Copy example environment file
cp .env.example .env

# Edit .env with your API keys and settings
nano .env  # or use your preferred editor
```

**Required environment variables:**

```bash
# API Keys (REQUIRED)
ANTHROPIC_API_KEY=sk-ant-xxxxx
GOOGLE_API_KEY=xxxxx
SERPAPI_KEY=xxxxx

# Database
DATABASE_URL=postgresql://user:password@localhost:5432/research_agent

# Application Settings
MAX_SEARCH_ITERATIONS=7
LOG_LEVEL=INFO
ENVIRONMENT=development

# Model Configuration
CLAUDE_MODEL=claude-sonnet-4-5-20250929
GEMINI_PRO_MODEL=gemini-2.5-pro
GEMINI_FLASH_MODEL=gemini-2.5-flash

# Performance Settings
MAX_CONCURRENT_SEARCH_CALLS=5
MAX_CONCURRENT_LLM_CALLS=10
EXTRACTION_BATCH_SIZE=5
```

### 4. Set Up Database

**Option A: Using Docker (Recommended)**

```bash
# Start PostgreSQL container
docker run -d \
  --name research-db \
  -e POSTGRES_PASSWORD=yourpassword \
  -e POSTGRES_DB=research_agent \
  -e POSTGRES_USER=research_user \
  -p 5432:5432 \
  postgres:14

# Wait for database to be ready (10 seconds)
sleep 10
```

**Option B: Use Existing PostgreSQL**

Ensure PostgreSQL is running and create a database:

```sql
CREATE DATABASE research_agent;
CREATE USER research_user WITH PASSWORD 'yourpassword';
GRANT ALL PRIVILEGES ON DATABASE research_agent TO research_user;
```

### 5. Run Database Migrations

```bash
# Create tables using Alembic
alembic upgrade head
```

### 6. Run the Application

```bash
# Start Chainlit UI
chainlit run src/ui/chainlit_app.py

# The UI will be available at http://localhost:8000
```

---

## ğŸ³ Docker Deployment

### Build Docker Image

```bash
# Build the image
docker build -t deep-research-agent -f Dockerfile .
```

### Run with Docker Compose (Recommended)

Create `docker-compose.yml`:

```yaml
version: '3.8'

services:
  postgres:
    image: postgres:14
    environment:
      POSTGRES_DB: research_agent
      POSTGRES_USER: research_user
      POSTGRES_PASSWORD: yourpassword
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U research_user"]
      interval: 10s
      timeout: 5s
      retries: 5

  app:
    image: deep-research-agent
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql://research_user:yourpassword@postgres:5432/research_agent
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      GOOGLE_API_KEY: ${GOOGLE_API_KEY}
      SERPAPI_KEY: ${SERPAPI_KEY}
    ports:
      - "8000:8000"
    volumes:
      - ./logs:/app/logs
      - ./reports:/app/reports

volumes:
  postgres_data:
```

Run with:

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f app

# Stop services
docker-compose down
```

### Run Docker Manually

```bash
# Run application container (after PostgreSQL is running)
docker run -d \
  --name research-agent \
  -p 8000:8000 \
  --env-file .env \
  -e DATABASE_URL=postgresql://research_user:yourpassword@host.docker.internal:5432/research_agent \
  deep-research-agent

# View logs
docker logs -f research-agent

# Stop container
docker stop research-agent
docker rm research-agent
```

---

## ğŸ“Š Performance Metrics

**Typical Performance (7 iterations):**

| Metric | Value |
|--------|-------|
| Execution Time | 5-7 minutes |
| Facts Discovered | 40-60 |
| API Calls | ~210 LLM calls |
| API Cost | $0.50-$1.00 |
| Source Diversity | 15-20 unique domains |
| Precision | >90% |
| Fact Discovery Rate | >70% |

**Optimizations Applied:**

- âœ… 66% reduction in API calls (new_facts strategy)
- âœ… 10x faster extraction (concurrent batch processing)
- âœ… Semantic deduplication (85% accuracy)
- âœ… AI-powered entity extraction (90-95% accuracy)
- âœ… Rate limiting for controlled concurrency

---

## ğŸ“ Project Structure

```
Elile-Assessment/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ graph.py              # LangGraph workflow orchestration
â”‚   â”‚   â”œâ”€â”€ state.py              # State schema (TypedDict)
â”‚   â”‚   â””â”€â”€ nodes/                # 7 specialized agent nodes
â”‚   â”‚       â”œâ”€â”€ planner.py        # Query generation (Gemini Flash)
â”‚   â”‚       â”œâ”€â”€ searcher.py       # Search execution (SerpApi)
â”‚   â”‚       â”œâ”€â”€ extractor.py      # Fact extraction (Gemini Pro)
â”‚   â”‚       â”œâ”€â”€ validator.py      # Validation (Claude Sonnet)
â”‚   â”‚       â”œâ”€â”€ risk_analyzer.py  # Risk detection (Claude Sonnet)
â”‚   â”‚       â”œâ”€â”€ connection_mapper.py  # Network mapping (Gemini Pro)
â”‚   â”‚       â””â”€â”€ reporter.py       # Report generation (Claude Sonnet)
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ models/               # AI model clients
â”‚   â”‚   â”‚   â”œâ”€â”€ anthropic_client.py
â”‚   â”‚   â”‚   â””â”€â”€ gemini_client.py
â”‚   â”‚   â””â”€â”€ search/               # Search integrations
â”‚   â”‚       â”œâ”€â”€ serp_api_search.py
â”‚   â”‚       â””â”€â”€ brave_search.py
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ models.py             # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ repository.py         # Data access layer
â”‚   â”‚   â””â”€â”€ migrations/           # Alembic migrations
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â””â”€â”€ templates/            # Prompt templates for each node
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ config.py             # Configuration management
â”‚   â”‚   â”œâ”€â”€ logger.py             # Structured logging (Loguru)
â”‚   â”‚   â””â”€â”€ rate_limiter.py       # Concurrent execution control
â”‚   â””â”€â”€ ui/
â”‚       â””â”€â”€ chainlit_app.py       # Chainlit web interface
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                     # Unit tests (mocked)
â”‚   â”œâ”€â”€ integration/              # Integration tests (real APIs)
â”‚   â””â”€â”€ evaluation/               # Test personas & metrics
â”œâ”€â”€ Dockerfile                    # Multi-stage Docker build
â”œâ”€â”€ docker-compose.yml            # Docker Compose configuration
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .env.example                  # Environment variable template
â”œâ”€â”€ alembic.ini                   # Database migration config
â””â”€â”€ README.md                     # This file
```

---

## ğŸ§ª Testing

```bash
# Run unit tests (no API calls)
pytest tests/unit/ -v

# Run integration tests (requires API keys)
pytest tests/integration/ -v

# Run specific test file
pytest tests/unit/test_nodes_planner_mock.py -v

# Run with coverage
pytest --cov=src tests/
```

---

## ğŸ”’ Security & Privacy

- âœ… API keys stored in environment variables (never committed)
- âœ… Rate limiting on all external API calls
- âœ… Input validation and sanitization
- âœ… Database connection via SQLAlchemy (SQL injection protection)

---

## ğŸ“ Example Usage

### Via Chainlit UI

1. Open browser to `http://localhost:8000`
2. Enter target name: `"Elon Musk"`
3. Select research depth: `7 iterations`
4. Click "Start Research"
5. Watch real-time progress updates
6. Download comprehensive report (Markdown/JSON)

### Via Python API

```python
from src.agents.graph import ResearchWorkflow
from src.database.repository import ResearchRepository
from src.utils.config import Config

# Initialize
config = Config.from_env()
repository = ResearchRepository(config.database.url)
workflow = ResearchWorkflow(repository, config)

# Run research
result = workflow.run_research(
    target_name="Elon Musk",
    research_depth=7
)

# Access results
print(result["final_report"])
print(f"Facts: {len(result['collected_facts'])}")
print(f"Risks: {len(result['risk_flags'])}")
print(f"Connections: {len(result['connections'])}")
```
