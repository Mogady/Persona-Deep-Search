# Deep Research AI Agent

An autonomous research agent for conducting comprehensive investigations on individuals or entities, uncovering hidden connections, potential risks, and strategic insights for due diligence operations.

## üéØ Overview

This AI agent conducts multi-layered investigations by:
- **Intelligently searching** the web using progressive query strategies
- **Extracting and validating** facts from multiple sources
- **Identifying risks** including legal, financial, and reputational concerns
- **Mapping connections** between entities, organizations, and events
- **Generating comprehensive reports** with confidence scoring

## ‚ú® Key Features

- **Multi-Model AI**: Uses Claude Sonnet 4.5, Gemini Pro 2.5, and Gemini 2.5 Flash for different tasks
- **Progressive Search**: Each iteration builds on previous findings using SerpApi (Google Search)
- **Source Validation**: Cross-references information across multiple sources
- **Risk Detection**: Automatically flags potential red flags
- **Connection Mapping**: Traces relationships and builds network graphs
- **Confidence Scoring**: Every fact rated for reliability
- **Interactive UI**: Real-time progress tracking via Chainlit

## üèóÔ∏è Architecture

```
User Input ‚Üí LangGraph Orchestrator ‚Üí 7 Specialized Nodes ‚Üí Comprehensive Report
```

### Agent Nodes
1. **Query Planner**: Generates intelligent search queries (Gemini 2.5 Flash)
2. **Search Executor**: Executes searches via SerpApi (Google Search)
3. **Content Extractor**: Extracts structured facts (Gemini Pro 2.5)
4. **Validator**: Cross-references and scores confidence (Claude Sonnet 4.5)
5. **Risk Analyzer**: Identifies red flags (Claude Sonnet 4.5)
6. **Connection Mapper**: Maps entity relationships (Gemini Pro 2.5)
7. **Report Generator**: Synthesizes findings (Claude Sonnet 4.5)

## üöÄ Quick Start

### Prerequisites
- Python 3.11+
- PostgreSQL 14+
- API Keys: Anthropic Claude, Google AI (Gemini), SerpApi

### Installation

```bash
# Clone repository
git clone <repository-url>
cd deep-research-agent

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install Playwright browsers (for web scraping)
playwright install chromium

# Setup environment variables
cp .env.example .env
# Edit .env with your API keys
```

### Database Setup

```bash
# Start PostgreSQL (or use Docker)
docker run -d \
  --name research-db \
  -e POSTGRES_PASSWORD=yourpassword \
  -e POSTGRES_DB=research_agent \
  -p 5432:5432 \
  postgres:14

# Run migrations
alembic upgrade head
```

### Run the Application

```bash
# Start Chainlit UI
chainlit run src/ui/chainlit_app.py

# Open browser to http://localhost:8000
```

## üìñ Usage

### Via UI

1. Open the Chainlit interface
2. Enter target name (person or entity)
3. Select research depth (1-7 iterations)
4. Click "Start Research"
5. Watch real-time progress
6. Download comprehensive report

### Via Python API

```python
from src.agents.graph import ResearchGraph
from src.utils.config import Config

# Initialize
config = Config.from_env()
graph = ResearchGraph(config)

# Run research
result = await graph.run_research(
    target_name="John Doe",
    max_iterations=7
)

# Access results
print(result["final_report"])
print(f"Facts found: {len(result['collected_facts'])}")
print(f"Risks identified: {len(result['risk_flags'])}")
```

## üìä Evaluation

The system includes 3 test personas with ground truth for evaluation:

```bash
# Run evaluation
python tests/evaluation/test_agent.py

# View metrics
# - Fact Discovery Rate: % of hidden facts found
# - Precision: % of facts correctly identified
# - Source Diversity: Unique domains used
# - Confidence Calibration: Score accuracy
# - Query Efficiency: Facts per search
```

**Target Metrics:**
- Fact Discovery Rate: >70%
- Precision: >90%
- Source Diversity: 10+ unique domains
- Execution Time: <10 minutes

## üîß Configuration

Edit `.env` file:

```bash
# API Keys (Required)
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=...
SERPAPI_KEY=...

# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/research_agent

# Application Settings
MAX_SEARCH_ITERATIONS=7
MAX_FACTS_PER_SESSION=100
RATE_LIMIT_REQUESTS_PER_MINUTE=60
LOG_LEVEL=INFO

# GCP (for deployment)
GCP_PROJECT_ID=your-project
GCP_STORAGE_BUCKET=research-reports
```

## üåê Deployment

### Docker

```bash
# Build image
docker build -t research-agent -f infra/docker/Dockerfile .

# Run container
docker run -p 8000:8000 \
  --env-file .env \
  research-agent
```

### GCP Cloud Run

```bash
# Setup GCP
cd infra/terraform
terraform init
terraform plan
terraform apply

# Deploy
gcloud run deploy research-agent \
  --image gcr.io/PROJECT_ID/research-agent \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated
```

## üìÅ Project Structure

```
deep-research-agent/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agents/          # LangGraph workflow and nodes
‚îÇ   ‚îú‚îÄ‚îÄ tools/           # AI models and search tools
‚îÇ   ‚îú‚îÄ‚îÄ database/        # Database models and repository
‚îÇ   ‚îú‚îÄ‚îÄ utils/           # Config, logger, rate limiter
‚îÇ   ‚îî‚îÄ‚îÄ ui/              # Chainlit interface
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/      # Test personas and metrics
‚îÇ   ‚îî‚îÄ‚îÄ unit/            # Unit tests
‚îú‚îÄ‚îÄ infra/
‚îÇ   ‚îú‚îÄ‚îÄ terraform/       # GCP infrastructure
‚îÇ   ‚îî‚îÄ‚îÄ docker/          # Container configuration
‚îú‚îÄ‚îÄ docs/                # Additional documentation
‚îú‚îÄ‚îÄ CLAUDE.md            # Project overview for AI agents
‚îú‚îÄ‚îÄ PROJECT_COORDINATION.md  # Development tracking
‚îú‚îÄ‚îÄ AGENT_PROMPTS.md     # Agent-specific instructions
‚îî‚îÄ‚îÄ README.md            # This file
```

## üß™ Testing

```bash
# Run unit tests
pytest tests/unit/

# Run integration tests
pytest tests/integration/

# Run evaluation
python tests/evaluation/test_agent.py

# Check code quality
flake8 src/
black src/ --check
mypy src/
```

## üìù Documentation

- **CLAUDE.md**: Comprehensive project overview
- **PROJECT_COORDINATION.md**: Development progress tracking
- **AGENT_PROMPTS.md**: Detailed instructions for each component
- **docs/ARCHITECTURE.md**: System architecture deep-dive
- **docs/API_REFERENCE.md**: API documentation
- **docs/DEPLOYMENT.md**: Deployment guide

## üîí Security & Privacy

- API keys stored in GCP Secret Manager
- Rate limiting on all external APIs
- No storage of sensitive personal data beyond session
- Audit logging for all searches
- GDPR considerations for personal information
- Respect for robots.txt and rate limits

## üõ†Ô∏è Development

### Adding a New Node

1. Create node class in `src/agents/nodes/`
2. Implement `execute(state: ResearchState)` method
3. Add to workflow in `src/agents/graph.py`
4. Create prompt template in `src/prompts/templates/`
5. Add tests in `tests/unit/`

### Debugging

```bash
# Enable debug logging
export LOG_LEVEL=DEBUG

# View execution logs
tail -f logs/research.log

# Inspect database
psql -d research_agent
SELECT * FROM research_sessions ORDER BY start_time DESC LIMIT 10;
```

## üìä Performance

**Typical Performance:**
- Execution Time: 5-7 minutes for 7 iterations
- API Costs: ~$0.50-$1.00 per research session
- Facts Discovered: 30-50 per session
- Source Diversity: 15-20 unique domains

**Optimization Tips:**
- Use caching for repeated searches
- Adjust max_iterations based on needs
- Use "basic" search depth for faster results
- Batch API calls where possible

## ü§ù Contributing

This is a technical assessment project. Guidelines:

1. Follow existing code structure
2. Add tests for new features
3. Update documentation
4. Use type hints and docstrings
5. Follow PEP 8 style guide

## üìú License

[To be determined]

## üôè Acknowledgments

- LangChain & LangGraph for orchestration framework
- Anthropic Claude for advanced reasoning and analysis
- Google Gemini (Pro & Flash) for extraction and query generation
- SerpApi for comprehensive search infrastructure

## üìû Support

For questions or issues:
- Check documentation in `docs/`
- Review example usage in `tests/`
- See PROJECT_COORDINATION.md for development status

## üéØ Roadmap

**Current Version**: 1.0 (MVP)

**Planned Features:**
- Multi-target batch processing
- Custom investigation templates
- API endpoint for programmatic access
- Advanced visualization (knowledge graphs)
- Export to multiple formats (PDF, JSON)
- Integration with external databases
- Real-time monitoring dashboard
- Scheduled investigations

---

**Built for**: Elile AI Technical Assessment
**Timeline**: 4-day development sprint
**Status**: [To be updated during development]