# ========================================
# Deep Research AI Agent - Environment Configuration
# ========================================

# ----------------------------------------
# AI Model API Keys (Required)
# ----------------------------------------
GOOGLE_API_KEY=your-google-api-key-here
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# ----------------------------------------
# Search API Keys (Required)
# ----------------------------------------
SERPAPI_KEY=your-serpapi-key-here

# Optional: Backup search APIs
# FIRECRAWL_API_KEY=your-firecrawl-api-key-here

# ----------------------------------------
# Database Configuration
# ----------------------------------------
DATABASE_URL=postgresql://user:password@localhost:5432/research_agent

# For Docker/Development
DB_USER=research_user
DB_PASSWORD=your_secure_password
DB_NAME=research_agent
DB_HOST=localhost
DB_PORT=5432

# ----------------------------------------
# Application Settings
# ----------------------------------------
# Maximum number of search iterations per research session
MAX_SEARCH_ITERATIONS=7

# Maximum facts to collect per session
MAX_FACTS_PER_SESSION=100

# Research timeout in minutes
RESEARCH_TIMEOUT_MINUTES=15

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Enable debug mode (true/false)
DEBUG_MODE=false

# ----------------------------------------
# Search Configuration
# ----------------------------------------
# Optional: Brave Search API key (secondary search engine)
# BRAVE_API_KEY=your-brave-api-key-here

# Search type (web/news/scholar)
SEARCH_TYPE=web

# Maximum search results per query
MAX_SEARCH_RESULTS_PER_QUERY=10

# Maximum concurrent search API calls
MAX_CONCURRENT_SEARCH_CALLS=10

# ----------------------------------------
# GCP Configuration (for deployment)
# ----------------------------------------
GCP_PROJECT_ID=your-gcp-project-id
GCP_REGION=us-central1

# GCP Secret Manager
USE_SECRET_MANAGER=false
SECRET_MANAGER_PROJECT=your-gcp-project-id

# GCP Cloud Storage
GCP_STORAGE_BUCKET=research-agent-reports
ENABLE_GCS_STORAGE=false

# Cloud SQL connection
CLOUD_SQL_CONNECTION_NAME=project:region:instance

# ----------------------------------------
# Model Configuration
# ----------------------------------------
# Gemini Pro Model (extraction, entity recognition, connection mapping)
GEMINI_PRO_MODEL=gemini-2.5-pro

# Gemini Flash Model (query generation, filtering, preliminary analysis)
GEMINI_FLASH_MODEL=gemini-2.5-flash

# Claude Sonnet Model (risk analysis, report generation, complex reasoning)
CLAUDE_MODEL=claude-sonnet-4-5-20250929


# ----------------------------------------
# Chainlit UI Configuration
# ----------------------------------------
CHAINLIT_HOST=0.0.0.0
CHAINLIT_PORT=8000
CHAINLIT_DEBUG=false

# ----------------------------------------
# Performance & Rate Limiting
# ----------------------------------------
# Maximum concurrent LLM API calls
MAX_CONCURRENT_LLM_CALLS=10

# Maximum concurrent search API calls (also set above)
# MAX_CONCURRENT_SEARCH_CALLS=10

# Extraction batch size
EXTRACTION_BATCH_SIZE=10

# API request timeout (seconds)
API_REQUEST_TIMEOUT=30

# API retry attempts
API_RETRY_ATTEMPTS=3

# ----------------------------------------
# Logging & Monitoring
# ----------------------------------------
# Log file path
LOG_FILE_PATH=logs/research.log

# Enable structured logging (JSON format)
STRUCTURED_LOGGING=true

# Log rotation
LOG_MAX_BYTES=10485760
LOG_BACKUP_COUNT=5

# Enable audit logging for searches
ENABLE_AUDIT_LOG=true
AUDIT_LOG_PATH=logs/audit.log
